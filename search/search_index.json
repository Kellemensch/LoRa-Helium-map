{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Lora-Helium-Map Documentation","text":"<p>For full project detail visit the GitHub page</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"README_docker/","title":"LoRa-Helium-Map \u2013 Installation &amp; Configuration Guide (Docker)","text":""},{"location":"README_docker/#requirements","title":"Requirements","text":"<p>Your computer or server must have an active internet connection to download dependencies and receive Helium network data.</p>"},{"location":"README_docker/#docker-installation","title":"Docker installation","text":"<p>This application requires Docker to run in a containerized environment. It is recommended to use a Debian-based OS (such as Ubuntu), but it can also work on Windows or macOS. See the documentation on how to install at : https://docs.docker.com/engine/install/</p> <p>Installation for Windows</p> <p>Or if you want a fast and automatic script which self download the proper version on your OS, see : get-docker</p> <p>Once installed, test Docker by running : <pre><code>docker --version\ndocker run --rm hello-world\n</code></pre></p>"},{"location":"README_docker/#helium-network-setup","title":"Helium network setup","text":"<p>To receive LoRa data, you must register your device with the Helium network.</p> <ol> <li>Register an account on the console : this is your main console where you can see all your registered devices and applications</li> <li>Go to <code>Applications</code> on the left side menu</li> <li>Click on <code>Add application</code> and fill the name you want</li> <li>Go to <code>Device Profiles</code> and click <code>Add</code>.</li> <li>Enter a name.</li> <li>Choose the correct MAC version (e.g. 1.0.3 for LoRa-E5).</li> <li>Choose the proper Regional Parameters (e.g. RP002-1.0.3 - EU868).</li> <li>Revision: B</li> <li>Expected uplink interval: <code>360</code></li> <li>Device-status request frequency: <code>1</code></li> <li>RX1 Delay: <code>0</code></li> <li>Enable \u201cFlush queue on activation\u201d</li> <li>Select the Default ADR algorithm</li> <li>Select OTAA or ABP, depending on your device</li> <li>Back in your Application, click <code>Add Device</code>.</li> <li>Enter a device name</li> <li>Provide the DevEUI (on your device label)</li> <li>Use <code>\"0000000000000000\"</code> as the Join EUI</li> <li>Choose the device profile you just created</li> <li>Click <code>Create</code></li> <li>Now, you need to configure your LoRa module to connect to the network using AT commands (you can generate a AppKey from your device in your application on the console), please put the good EUI and AppKey :</li> </ol> <p><pre><code>AT+ID=DevEui,\"put your EUI here\"\nAT+KEY=APPKEY,\"put your AppKey here\"\nAT+DR=EU868 \\\\ OR 915 for USA\nAT+JOIN\n</code></pre> 8.  Upon succes, you should see : <pre><code>+JOIN: Network joined successfully\n</code></pre></p>"},{"location":"README_docker/#http-integration-in-helium-console","title":"HTTP integration in Helium Console","text":"<p>Now that you have registered your device on the Helium Network, you will need to create an HTTP integration in order to retrieve data with the LoRa-Helium-map application.</p> <ol> <li>In the Helium Console, go to <code>Applications</code> \u2192 your app \u2192 <code>Integrations</code>.</li> <li>Click <code>Add Integration</code>, then choose <code>HTTP</code>.</li> <li>Set:</li> <li>Payload encoding: <code>JSON</code></li> <li>Endpoint URL: your localtunnel URL, ending with <code>/helium-data</code>      Example: <code>https://yourlaboratory.loca.lt/helium-data</code></li> <li>Headers:<ul> <li>Key: <code>Content-Type</code></li> <li>Value: <code>application/json</code></li> </ul> </li> <li>Click <code>Create Integration</code></li> </ol>"},{"location":"README_docker/#download","title":"Download","text":"<p>Download the latest version here and unzip the package.</p>"},{"location":"README_docker/#launch","title":"Launch","text":"<p>To launch the application you must know your end-device's latitude and longitude in degrees, and you url subdomain (in the example above : yourlaboratory)</p> <p>If you are on Linux, you can then launch the application by simply typing : <pre><code>./run.sh\n</code></pre> You will be asked to provide the latitude, longitude and subdomain.</p> <p>It will then automatically download terrain files around your end-device's place and begin to retrieve data from the Helium Network.</p> <p>It is recommended to leave the application running all day and night long on a running server in order to get the maximum of data.</p>"},{"location":"README_docker/#results","title":"Results","text":"<p>Results can be found in the <code>output/</code> folder in your current directory.</p> <p>The application writes every 5 minutes a map in <code>output/map.html</code> using the Splat! tool.</p> <p>It also runs twice a day (every 12 hours) an IGRA calculation using radiosonde balloons observations. It is important to note that new data from this dataset is not necessary available everyday, so you might wait a few days before new data are available and processed by the application.</p> <p>Be aware that all the dataset retrieved from you Helium end-node is stored in <code>output/data/helium_gateway_data.csv</code> and that no other version of this file does exist elsewhere. You might want to do regular saves of this file on another disk for backup (old data cannot be retrieved anymore).</p> <p>Finally, you can see the resulted map on your dedicated website ! It is available at the address with the subdomain you chose : <code>http://subdomain.loca.lt/map</code>, for example : <code>http://yourlaboratory.loca.lt/map</code></p> <p>Logs are available with the command : <code>docker logs -f lora-map</code> You can stop the application with the command : <code>docker compose down</code></p> <p>A container named <code>watchtower</code> is used to pull updates of this application from the cloud. If an update is available, the application will be restarted without destroying any file and dataset.</p> <p>If the tunnel is unavailable, don't panic, it will be relaunched automatically by the application. It is probably due to localtunnel's servers, it might take some time. If the problem still persists, be sure that your connection to internet is authorized.</p>"},{"location":"docker_documentation/","title":"Docker Documentation","text":"<p>The Docker repository can be found in DockerHub here.</p>"},{"location":"docker_documentation/#containers-used","title":"Containers used","text":"<p>This project uses a total of 2 containers:</p> <ul> <li>The application <code>app</code> : from main repository <code>kellemensch/lora-helium-map:latest</code></li> <li>A Watchtower to automate updates: from <code>containrrr/watchtower</code> - See GitHub repo</li> </ul>"},{"location":"docker_documentation/#available-files","title":"Available files","text":""},{"location":"docker_documentation/#docker-composeyaml","title":"docker-compose.yaml","text":"<pre><code>services:\n  app:\n    build:\n      context: .\n      args:\n        USER_ID: ${LOCAL_UID}\n        GROUP_ID: ${LOCAL_GID}\n    image: kellemensch/lora-helium-map:latest\n    container_name: lora-map\n    ports:\n      - \"5000:5000\"\n    volumes:\n      - ./output:/app/output\n      - ./configs:/app/configs\n    restart: unless-stopped\n\n  watchtower:\n    image: containrrr/watchtower\n    container_name: watchtower\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    environment:\n      - WATCHTOWER_CLEANUP=true\n      - WATCHTOWER_POLL_INTERVAL=300  # Every 5 minutes\n    restart: unless-stopped\n</code></pre> <p>The <code>docker-compose</code> file starts these two containers by making sure the output files created by the application are user files and not root files (in order to keep it rootless). </p> <p>Then it creates two volumes:</p> <ul> <li><code>output/</code> folder to save and plot the results</li> <li><code>configs/</code> folder containing the python module that returns the end-node's coordinates</li> </ul>"},{"location":"docker_documentation/#runsh","title":"run.sh","text":"<p>This scripts automates the configuration for the docker. It makes you create the coordinates of your end-node and the subdomain you want to use for your webhook (same as the one in Helium integration) and for the website exposure.</p> <p>The coordinates are written into <code>configs/.latitude</code> and <code>configs/.longitude</code>, and the subdomain into <code>configs/.subdomain</code>.</p> <p>Then it sets two environment variables <code>LOCAL_UID</code> and <code>LOCAL_GID</code> so that Docker can retrieve them from the docker-compose.</p> <p>Finally, it launches the pull from DockerHub and starts the containers.</p>"},{"location":"docker_documentation/#files-in-docker-application-image","title":"Files in Docker application image","text":""},{"location":"docker_documentation/#mainpy","title":"main.py","text":"<p>The main script running all processes. At first launch, it runs all the useful files:</p> <ul> <li><code>download_terrain</code> and <code>convert_hgt_to_sdf</code>: to download and convert all the files needed for Splat!</li> <li><code>run_localtunnel</code>: to prepare and run the LocalTunnel with the given subdomain</li> <li><code>webhook_server</code>: to create the Flask server linked to the LocalTunnel</li> <li><code>calculate_igra</code>: to compute IGRA calculations and downloads</li> <li><code>era5_gradients</code>: to compute ERA5 calculations and downloads</li> <li><code>run_splat</code> and <code>generate_maps</code>: to make Splat! computations and generate the map.html </li> <li><code>main_stats</code>: to calculate statistics and plot graphs</li> </ul> <p>This script stores the processes launched to ensure a clean shutdown without leaving any processes still running or blocked.</p> <p>Then it fixes the ownership of folder <code>output</code> created to be owned by actual user.</p> <p>Calculations are scheduled and relaunched several times in throughout the day:</p> <ul> <li>Every 5 minutes for the Splat! computation and map.html writing (because end-node receives every 5 minutes)</li> <li>Twice a day (every 12 hours) for the IGRA computation</li> <li>Once a day (every 24 hours) for the ERA5 computation and for the statistics</li> </ul>"},{"location":"docker_documentation/#download_terrainpy","title":"download_terrain.py","text":"<p>This script is the first one launched and is used to download useful files (<code>.sdf</code> files) in order to the good use of Splat!.</p> <p>The Shuttle Radar Topography Mission (SRTM) files are all downloaded from https://www.usgs.gov/ by this mirror site as the USGS dataset is now unavailable.</p> <p>The script first take all tiles around for 3 degrees and guesses the region of the end-node (Africa, Australia, Eurasia, Islands, North-America, South-America) in order to go to the right directory for download.</p> <p>Then it downloads files named for example <code>N16E073.hgt.zip</code> at https://srtm.kurviger.de/SRTM3/Eurasia/N16E073.hgt.zip and unzip them under <code>/app/maps</code>.</p>"},{"location":"docker_documentation/#convert_hgt_to_sdfsh","title":"convert_hgt_to_sdf.sh","text":"<p>Used to convert the files downloaded by <code>download_terrain.py</code> into <code>SDF</code> (SPLAT Data Files) under <code>/app/maps</code> using the tool <code>srtm2sdf</code> from Splat!. (See Splat! Documentaion)</p> <p>See How to use splat</p>"},{"location":"docker_documentation/#run_localtunnelsh","title":"run_localtunnel.sh","text":"<p>Launches and maintain the LocalTunnel.</p> <p>Runs continuously and checks every minute if the tunnel is working by curling the URL, if not it then relaunches the tunnel.</p> <p>The tunnel is working on <code>localhost:5000</code>.</p>"},{"location":"docker_documentation/#webhook_serverpy","title":"webhook_server.py","text":"<p>This is the core of the application: the Flask server used as a webhook to retrieve data from https://console.helium-iot.xyz and to serve the generated maps online through the tunnel.</p> <p>It was first implemented by Marco RAINONE to test the retrieval of Helium data.</p> <p>It serves a server on <code>localhost:5000</code>. Helium data are automatically retrieved by using the HTTP integration (see doc) on the endpoint <code>helium-data</code> and filled in the <code>data/helium_gateway_data.csv</code>. The home route <code>/</code> only returns 'OK' to test the proper functioning with the previous script.</p> <p>A static map.html created can be served on <code>/map</code> but it is deprecated. Prefer using the route <code>/dynamic-map</code> that returns a simple HTML file using JavaScript fetchs to call the <code>/api</code> endpoint and retrieving useful data from the dataset.</p> <p>The script <code>dynamic_map.js</code> first fetch the configuration (coordinates of end-node) and all the gateway links, but in an optimized way, as well as the IGRA stations to show.</p> <p>This script also serves a route <code>/logs</code> to download the logs file of the application.</p> <p>Statistics are available at <code>/stats</code>.</p> <p>All other routes are used by the JavaScript to serve data or graphs.</p> <p>Here are all the routes created:</p> <ul> <li>/helium-data</li> <li>/api<ul> <li>/api/optimized_gateways</li> <li>/api/dates</li> <li>/api/igra_stations</li> <li>/api/gateways</li> <li>/api/config</li> <li>/api/era5_graph : used with arguments (lat, lon, date, time)</li> <li>/api/era5_daily_graph : used with argument (date)</li> </ul> </li> <li>/dynamic-map</li> <li>/map</li> <li>/app/output/igra-datas/derived/&lt;path:filename&gt;</li> <li>/plots/&lt;path:filename&gt;</li> <li>/plots</li> <li>/stats</li> </ul> <p>Note: Fetching the requested data for the requested day every time may take huge time and is not optimized. That is why the script is fetching all data once.</p>"},{"location":"docker_documentation/#calculate_igrapy","title":"calculate_igra.py","text":"<p>For detailled information about Integrated Global Radiosonde Archive (IGRA) version 2.2 see the doc and README. Here, the 'derived' section of this dataset is used as it gives directly refractivity measurements for each height.</p> <p>This script is used to automate the download of all useful data from FTP directory from National Centers for Environmental Information (NCEI). At the start of each launch, it makes sure to delete all old files in order to get the latest data. It gets the <code>igra2-station-list.txt</code> which lists all the station available and their last updates. Then, for each row of the dataset (corresponding to each link with gateways) it computes the spherical midpoint between this gateway and the end-node in order to find the nearest IGRA radiosonde to this point and download its latest data. The file contains all data from the beginning of the radiosonde's life until the last update (usually the day before), so this script parse the file to get all data for refractivity for all heights at the requested date.</p> <p>Finally, gradients are calculated and plotted into <code>output/igra-datas/derived/</code>, and a utility file <code>output/igra-datas/map_links.json</code> is created. This file is used to better and easily generate the static map.html (deprecated) by linking all gateways to their coordinates, nearest station, and URL path for graphs for all days. Its structure is like: <pre><code>{\n    \"gatewayID\": {\n        \"gateway_name\": name,\n        \"gateway_coords\": [lat, lon],\n        \"station_id\": IGRA_id,\n        \"station_coords\": [lat, lon],\n        \"midpoint\": [lat, lon],\n        \"graphs\": {\n            \"2025-06-06\": \"/app/output/igra-datas/derived/gradient_gatewayName_2025-06-06.png\",\n            ...\n        }\n    },\n    ...\n}\n</code></pre></p> <p>Graph plotted contain a small description on their values about different ducting cases (see <code>describe_ducting_case()</code>).</p> <p>A cache file <code>output/processed_gradients.json</code> is used to store already processed links in our dataset, so that for each relaunch, the script only compute new data. Its structure is something like: <pre><code>{\n    \"gatewayName1\": [\n        \"date1\",\n        \"date2\",\n        ...\n    ],\n    \"gatewayName2\": [\n        \"date1\",\n        ...\n    ],\n    ...\n}\n</code></pre></p> <p>Links that are noted <code>LOS</code> (in Line-Of-Sight) are skipped here as their graph are not useful in this research.</p>"},{"location":"docker_documentation/#era5_gradientspy","title":"era5_gradients.py","text":"<p>This script is the latest to have been created. It computes refractivity gradients in addition of IGRA data. It was thought to add more precise data in term of localisation and time as there are not so many IGRA stations.</p>"}]}